{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import *\n",
    "from lstm.imdb_lstm import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "We reload the dataset with the plain text plots and the labels that reef generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(labels 1920\n"
     ]
    }
   ],
   "source": [
    "dataset='imdb'\n",
    "\n",
    "from data.imdb_loader import DataLoader\n",
    "# from data.youtube_loader import DataLoader\n",
    "# from data.trec_loader import DataLoader\n",
    "# from data.sms_loader import DataLoader\n",
    "\n",
    "dl = DataLoader()\n",
    "train_primitive_matrix, val_primitive_matrix, test_primitive_matrix, train_ground, val_ground, \\\n",
    "test_ground, _, _, _, train_text, val_text, test_text = dl.load_data(dataset=dataset, split_val = 0.1)\n",
    "# _, _, _, train_ground, val_ground, test_ground, train_text, val_text, test_text = dl.load_data(dataset=dataset)\n",
    "# _, _, _, train_ground, val_ground, test_ground, _,_,_,train_text, val_text, test_text= dl.load_data(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'LFs/'+dataset+'/val_5_dict_dt1/normal_reef.npy'\n",
    "train_reef = np.load(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an LSTM Model\n",
    "We now train a simple LSTM model with the labels generated by Reef. The following hyperparameter search is simplistic, and a more fine-tuned search and a more complex model can improve performance!\n",
    "\n",
    "__Note that this takes ~1 hour to run on CPU__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.5       , 0.5       , ..., 0.5       , 0.10919017,\n",
       "       0.5       ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0509 13:13:41.652681 140160707962688 deprecation_wrapper.py:119] From /home/ayusham/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0509 13:13:41.669822 140160707962688 deprecation_wrapper.py:119] From /home/ayusham/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0509 13:13:41.673306 140160707962688 deprecation_wrapper.py:119] From /home/ayusham/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0509 13:13:42.214559 140160707962688 deprecation_wrapper.py:119] From /home/ayusham/.local/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0509 13:13:42.231076 140160707962688 deprecation_wrapper.py:119] From /home/ayusham/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0509 13:13:42.235513 140160707962688 deprecation.py:323] From /home/ayusham/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           226240    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 279,541\n",
      "Trainable params: 279,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0509 13:13:42.861724 140160707962688 deprecation_wrapper.py:119] From /home/ayusham/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1278 samples, validate on 142 samples\n",
      "Epoch 1/25\n",
      "1278/1278 [==============================] - 15s 11ms/step - loss: 0.6844 - acc: 0.0000e+00 - val_loss: 0.6879 - val_acc: 0.5282\n",
      "Epoch 2/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6761 - acc: 0.0000e+00 - val_loss: 0.6685 - val_acc: 0.5282\n",
      "Epoch 3/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6482 - acc: 0.0000e+00 - val_loss: 0.5922 - val_acc: 0.6408\n",
      "Epoch 4/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6275 - acc: 0.0000e+00 - val_loss: 0.5823 - val_acc: 0.6831\n",
      "Epoch 5/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6164 - acc: 0.0000e+00 - val_loss: 0.5685 - val_acc: 0.6831\n",
      "Epoch 6/25\n",
      "1278/1278 [==============================] - 13s 10ms/step - loss: 0.6113 - acc: 0.0000e+00 - val_loss: 0.5623 - val_acc: 0.6972\n",
      "Epoch 7/25\n",
      "1278/1278 [==============================] - 13s 10ms/step - loss: 0.6087 - acc: 0.0000e+00 - val_loss: 0.5648 - val_acc: 0.6761\n",
      "Epoch 8/25\n",
      "1278/1278 [==============================] - 13s 10ms/step - loss: 0.6072 - acc: 0.0000e+00 - val_loss: 0.5628 - val_acc: 0.6761\n",
      "Epoch 9/25\n",
      "1278/1278 [==============================] - 13s 10ms/step - loss: 0.6057 - acc: 0.0000e+00 - val_loss: 0.5629 - val_acc: 0.6761\n",
      "Epoch 10/25\n",
      "1278/1278 [==============================] - 13s 10ms/step - loss: 0.6045 - acc: 0.0000e+00 - val_loss: 0.5559 - val_acc: 0.6831\n",
      "Epoch 11/25\n",
      "1278/1278 [==============================] - 13s 10ms/step - loss: 0.6043 - acc: 0.0000e+00 - val_loss: 0.5567 - val_acc: 0.6831\n",
      "Epoch 12/25\n",
      "1278/1278 [==============================] - 13s 11ms/step - loss: 0.6039 - acc: 0.0000e+00 - val_loss: 0.5582 - val_acc: 0.6901\n",
      "Epoch 13/25\n",
      "1278/1278 [==============================] - 13s 10ms/step - loss: 0.6037 - acc: 0.0000e+00 - val_loss: 0.5569 - val_acc: 0.6901\n",
      "Epoch 14/25\n",
      "1278/1278 [==============================] - 13s 10ms/step - loss: 0.6035 - acc: 0.0000e+00 - val_loss: 0.5571 - val_acc: 0.6831\n",
      "Epoch 15/25\n",
      "1278/1278 [==============================] - 13s 10ms/step - loss: 0.6034 - acc: 0.0000e+00 - val_loss: 0.5553 - val_acc: 0.6831\n",
      "Epoch 16/25\n",
      "1278/1278 [==============================] - 13s 10ms/step - loss: 0.6033 - acc: 0.0000e+00 - val_loss: 0.5570 - val_acc: 0.6901\n",
      "Epoch 17/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6033 - acc: 0.0000e+00 - val_loss: 0.5552 - val_acc: 0.6901\n",
      "Epoch 18/25\n",
      "1278/1278 [==============================] - 13s 11ms/step - loss: 0.6033 - acc: 0.0000e+00 - val_loss: 0.5583 - val_acc: 0.6831\n",
      "Epoch 19/25\n",
      "1278/1278 [==============================] - 13s 10ms/step - loss: 0.6033 - acc: 0.0000e+00 - val_loss: 0.5565 - val_acc: 0.6831\n",
      "Epoch 20/25\n",
      "1278/1278 [==============================] - 13s 10ms/step - loss: 0.6033 - acc: 0.0000e+00 - val_loss: 0.5565 - val_acc: 0.6831\n",
      "Epoch 21/25\n",
      "1278/1278 [==============================] - 13s 10ms/step - loss: 0.6033 - acc: 0.0000e+00 - val_loss: 0.5566 - val_acc: 0.6901\n",
      "Epoch 22/25\n",
      "1278/1278 [==============================] - 13s 10ms/step - loss: 0.6033 - acc: 0.0000e+00 - val_loss: 0.5564 - val_acc: 0.6901\n",
      "Epoch 23/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6033 - acc: 0.0000e+00 - val_loss: 0.5563 - val_acc: 0.6901\n",
      "Epoch 24/25\n",
      "1278/1278 [==============================] - 13s 10ms/step - loss: 0.6034 - acc: 0.0000e+00 - val_loss: 0.5530 - val_acc: 0.7183\n",
      "Epoch 25/25\n",
      "1278/1278 [==============================] - 13s 10ms/step - loss: 0.6034 - acc: 0.0000e+00 - val_loss: 0.5594 - val_acc: 0.6901\n",
      "Accuracy: 69.01%\n"
     ]
    }
   ],
   "source": [
    "f1_all = []\n",
    "pr_all = []\n",
    "re_all = []\n",
    "val_acc_all = []\n",
    "\n",
    "\n",
    "bs_arr = [64]#,128,256]\n",
    "n_epochs_arr = [25]#[5,10,25]\n",
    "\n",
    "for bs in bs_arr:\n",
    "    for n in n_epochs_arr:\n",
    "        y_pred = lstm_simple(train_text, train_reef, val_text, val_ground, bs=bs, n=n)\n",
    "#         print(y_pred)\n",
    "        predictions = np.round(y_pred)\n",
    "        \n",
    "        val_acc_all.append(np.sum(predictions == val_ground)/float(np.shape(val_ground)[0]))\n",
    "        f1_all.append(metrics.f1_score(val_ground, predictions,average=\"macro\"))\n",
    "        pr_all.append(metrics.precision_score(val_ground, predictions))\n",
    "        re_all.append(metrics.recall_score(val_ground, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Batch Size:  64\n",
      "Best Epochs:  25\n",
      "Validation F1 Score:  0.6756644518272426\n",
      "Validation Best Pr:  0.7555555555555555\n",
      "Validation Best Re:  0.5074626865671642\n"
     ]
    }
   ],
   "source": [
    "ii,jj = np.unravel_index(np.argmax(f1_all), (1,1))\n",
    "print('Best Batch Size: ', bs_arr[ii])\n",
    "print('Best Epochs: ', n_epochs_arr[jj])\n",
    "\n",
    "print('Validation F1 Score: ', max(f1_all))\n",
    "print('Validation Best Pr: ', pr_all[np.argmax(f1_all)])\n",
    "print('Validation Best Re: ', re_all[np.argmax(f1_all)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Performance\n",
    "We re-train the model with the best validation performance since we don't save weights for the models currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 32)           256288    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 309,589\n",
      "Trainable params: 309,589\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1278 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "1278/1278 [==============================] - 16s 12ms/step - loss: 0.6856 - acc: 0.0000e+00 - val_loss: 0.6907 - val_acc: 0.5260\n",
      "Epoch 2/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6782 - acc: 0.0000e+00 - val_loss: 0.6813 - val_acc: 0.5260\n",
      "Epoch 3/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6495 - acc: 0.0000e+00 - val_loss: 0.6078 - val_acc: 0.7040\n",
      "Epoch 4/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6262 - acc: 0.0000e+00 - val_loss: 0.6332 - val_acc: 0.6500\n",
      "Epoch 5/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6167 - acc: 0.0000e+00 - val_loss: 0.6168 - val_acc: 0.6700\n",
      "Epoch 6/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6112 - acc: 0.0000e+00 - val_loss: 0.6169 - val_acc: 0.6600\n",
      "Epoch 7/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6088 - acc: 0.0000e+00 - val_loss: 0.6225 - val_acc: 0.6680\n",
      "Epoch 8/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6074 - acc: 0.0000e+00 - val_loss: 0.6234 - val_acc: 0.6600\n",
      "Epoch 9/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6059 - acc: 0.0000e+00 - val_loss: 0.6203 - val_acc: 0.6660\n",
      "Epoch 10/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6052 - acc: 0.0000e+00 - val_loss: 0.6187 - val_acc: 0.6400\n",
      "Epoch 11/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6084 - acc: 0.0000e+00 - val_loss: 0.6408 - val_acc: 0.6260\n",
      "Epoch 12/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6127 - acc: 0.0000e+00 - val_loss: 0.6282 - val_acc: 0.6580\n",
      "Epoch 13/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6084 - acc: 0.0000e+00 - val_loss: 0.6285 - val_acc: 0.6480\n",
      "Epoch 14/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6057 - acc: 0.0000e+00 - val_loss: 0.6224 - val_acc: 0.6500\n",
      "Epoch 15/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6043 - acc: 0.0000e+00 - val_loss: 0.6231 - val_acc: 0.6540\n",
      "Epoch 16/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6036 - acc: 0.0000e+00 - val_loss: 0.6259 - val_acc: 0.6600\n",
      "Epoch 17/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6034 - acc: 0.0000e+00 - val_loss: 0.6224 - val_acc: 0.6440\n",
      "Epoch 18/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6033 - acc: 0.0000e+00 - val_loss: 0.6243 - val_acc: 0.6560\n",
      "Epoch 19/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6032 - acc: 0.0000e+00 - val_loss: 0.6238 - val_acc: 0.6500\n",
      "Epoch 20/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6031 - acc: 0.0000e+00 - val_loss: 0.6237 - val_acc: 0.6460\n",
      "Epoch 21/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6031 - acc: 0.0000e+00 - val_loss: 0.6236 - val_acc: 0.6500\n",
      "Epoch 22/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6031 - acc: 0.0000e+00 - val_loss: 0.6244 - val_acc: 0.6480\n",
      "Epoch 23/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6031 - acc: 0.0000e+00 - val_loss: 0.6225 - val_acc: 0.6500\n",
      "Epoch 24/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6031 - acc: 0.0000e+00 - val_loss: 0.6246 - val_acc: 0.6480\n",
      "Epoch 25/25\n",
      "1278/1278 [==============================] - 14s 11ms/step - loss: 0.6031 - acc: 0.0000e+00 - val_loss: 0.6234 - val_acc: 0.6460\n",
      "Accuracy: 64.60%\n"
     ]
    }
   ],
   "source": [
    "y_pred = lstm_simple(train_text, train_reef, test_text, test_ground, bs=bs_arr[ii], n=n_epochs_arr[jj])\n",
    "predictions = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 Score:  0.6231974147510762\n",
      "Test Precision:  0.7142857142857143\n",
      "Test Recall:  0.4219409282700422\n"
     ]
    }
   ],
   "source": [
    "print ('Test F1 Score: ', metrics.f1_score(test_ground, predictions, average='macro'))\n",
    "print ('Test Precision: ', metrics.precision_score(test_ground, predictions))\n",
    "print ('Test Recall: ', metrics.recall_score(test_ground, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Ground Truth Performance\n",
    "We can also train the same model with ground truth labels for the train set to see how far Reef labels are from the best possible performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 32)           256288    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 309,589\n",
      "Trainable params: 309,589\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1278 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "1278/1278 [==============================] - 9s 7ms/step - loss: 0.6896 - acc: 0.5383 - val_loss: 0.6889 - val_acc: 0.5260\n",
      "Epoch 2/10\n",
      "1278/1278 [==============================] - 8s 6ms/step - loss: 0.6761 - acc: 0.5415 - val_loss: 0.6758 - val_acc: 0.5260\n",
      "Epoch 3/10\n",
      "1278/1278 [==============================] - 8s 6ms/step - loss: 0.6426 - acc: 0.6706 - val_loss: 0.6346 - val_acc: 0.7860\n",
      "Epoch 4/10\n",
      "1278/1278 [==============================] - 8s 6ms/step - loss: 0.6002 - acc: 0.9218 - val_loss: 0.6397 - val_acc: 0.7560\n",
      "Epoch 5/10\n",
      "1278/1278 [==============================] - 8s 6ms/step - loss: 0.5798 - acc: 0.9241 - val_loss: 0.6207 - val_acc: 0.7680\n",
      "Epoch 6/10\n",
      "1278/1278 [==============================] - 8s 6ms/step - loss: 0.5280 - acc: 0.9335 - val_loss: 0.5829 - val_acc: 0.7800\n",
      "Epoch 7/10\n",
      "1278/1278 [==============================] - 8s 6ms/step - loss: 0.4303 - acc: 0.9429 - val_loss: 0.5063 - val_acc: 0.8000\n",
      "Epoch 8/10\n",
      "1278/1278 [==============================] - 8s 6ms/step - loss: 0.2782 - acc: 0.9570 - val_loss: 0.3994 - val_acc: 0.8360\n",
      "Epoch 9/10\n",
      "1278/1278 [==============================] - 8s 6ms/step - loss: 0.1648 - acc: 0.9656 - val_loss: 0.3736 - val_acc: 0.8580\n",
      "Epoch 10/10\n",
      "1278/1278 [==============================] - 8s 6ms/step - loss: 0.1280 - acc: 0.9609 - val_loss: 0.7764 - val_acc: 0.6920\n",
      "Accuracy: 69.20%\n"
     ]
    }
   ],
   "source": [
    "y_pred = lstm_simple(train_text, train_ground, test_text, test_ground, bs=128, n=10)\n",
    "predictions = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 Score:  0.6461397058823529\n",
      "Test Precision:  1.0\n",
      "Test Recall:  0.350210970464135\n"
     ]
    }
   ],
   "source": [
    "print ('Test F1 Score: ', metrics.f1_score(test_ground, predictions, average='macro'))\n",
    "print ('Test Precision: ', metrics.precision_score(test_ground, predictions))\n",
    "print ('Test Recall: ', metrics.recall_score(test_ground, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
