{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import json\n",
    "from sklearn import model_selection as cross_validation\n",
    "import re\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "def parse_file(filename):\n",
    "\n",
    "    def parse(filename):\n",
    "        tweet = []\n",
    "        print(filename)\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "#                 print(line)\n",
    "                tweet.append(line)\n",
    "        return tweet\n",
    "\n",
    "    tweets = parse(filename)\n",
    "    gt = []\n",
    "    plots = []\n",
    "    idx = []\n",
    "    for i,twt in enumerate(tweets):\n",
    "        tweet = twt.split(':')\n",
    "#         print(tweet)\n",
    "        genre = tweet[0]\n",
    "        tweet_txt = tweet[1]\n",
    "#         tweet_txt = re.sub(r\"@\\w+\",\"\", tweet[1])\n",
    "#         tweet_txt = ' '.join(tweet_txt.split(' ')[3:])\n",
    "        \n",
    "        if 'NUM' in genre:\n",
    "            plots.append(tweet_txt)\n",
    "            gt.append(0)\n",
    "            idx.append(i)\n",
    "        elif 'LOC' in genre:\n",
    "            plots.append(tweet_txt)\n",
    "            gt.append(1)\n",
    "            idx.append(i)\n",
    "        elif 'HUM' in genre:\n",
    "            plots.append(tweet_txt)\n",
    "            gt.append(2)\n",
    "            idx.append(i)\n",
    "        elif 'DESC' in genre:\n",
    "            plots.append(tweet_txt)\n",
    "            gt.append(3)\n",
    "            idx.append(i)\n",
    "        elif 'ENTY' in genre:\n",
    "            plots.append(tweet_txt)\n",
    "            gt.append(4)\n",
    "            idx.append(i)\n",
    "        elif 'ABBR' in genre:\n",
    "            plots.append(tweet_txt)\n",
    "            gt.append(5)\n",
    "            idx.append(i)\n",
    "        else:\n",
    "            continue  \n",
    "\n",
    "    print('len of data',len(plots))\n",
    "    return np.array(plots), np.array(gt)\n",
    "\n",
    "def split_data(plots, y):\n",
    "    np.random.seed(1234)\n",
    "#     num_sample = np.shape(X)[0]\n",
    "    num_test = 500\n",
    "    plots, y  = shuffle( plots, y, random_state = 25)\n",
    "#     X_test = X[0:num_test,:]\n",
    "#     X_train = X[num_test:, :]\n",
    "    plots_train = plots[num_test:]\n",
    "    plots_test = plots[0:num_test]\n",
    "\n",
    "    y_test = y[0:num_test]\n",
    "    y_train = y[num_test:]\n",
    "\n",
    "    # split dev/test\n",
    "    test_ratio = 0.05\n",
    "    y_tr, y_te, plots_tr, plots_te =cross_validation.train_test_split(y_train, plots_train, test_size = test_ratio, random_state=25)\n",
    "\n",
    "    return np.array(y_te), np.array(y_test), plots_te, plots_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/trec/all.txt\n",
      "len of data 5965\n"
     ]
    }
   ],
   "source": [
    "data_path='data/trec/'\n",
    "plots, labels = parse_file(data_path+'all.txt')\n",
    "y_valid, y_test, plots_valid, plots_test =  split_data(plots,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what sprawling u.s. state boasts the most airports ?\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plots[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [('did','you'), ('develop', 'kyun')]\n",
    "\n",
    "for i in x:\n",
    "    plots[0] = plots[0].replace(i[0],i[1])\n",
    "print(plots[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NUM', 1: 'LOC', 2: 'HUM', 3: 'DESC', 4: 'ENTY', 5: 'ABBR'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LOC'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plots[0]\n",
    "\n",
    "mapp = {0:'NUM',1:'LOC',2:'HUM',3:'DESC', 4:'ENTY', 5:'ABBR'}\n",
    "\n",
    "mapp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = nlp(plots)\n",
    "phrases = set()\n",
    "writestr = ''\n",
    "for i in range(len(labels)):\n",
    "    doc = nlp(str(plots[i]))\n",
    "    rep_arr = []\n",
    "    plots[i] = plots[i].replace(\"'\",'').replace(\"`\",'').replace(\":\",\"\")\n",
    "    plots[i] = plots[i].strip()\n",
    "#     print('inside',plots[i])\n",
    "    for e in doc.ents:\n",
    "        rep_arr.append((e.text, e.label_))\n",
    "#         print(e.text, e.start_char, e.end_char, e.label_)\n",
    "#     print(rep_arr)\n",
    "    if len(rep_arr) > 0:\n",
    "        for rep in rep_arr:\n",
    "            replac = rep[0] + \" \" + rep[1]\n",
    "            plots[i] = plots[i].replace(rep[0], replac )\n",
    "    writestr += str(mapp[labels[i]]) + \":\" + str(plots[i]) +'\\n'\n",
    "with open('data/trec/trec_ent_replaced_or.txt','w') as f:\n",
    "    f.write(writestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when did spielberg direct `` jaws \n"
     ]
    }
   ],
   "source": [
    "x = \"when did spielberg direct `` jaws ''\" \n",
    "print(x.replace(\"'\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "def create_ngram_features(words, n=2):\n",
    "    ngram_vocab = ngrams(words, n)\n",
    "    my_dict = dict([(ng, True) for ng in ngram_vocab])\n",
    "    return my_dict\n",
    "\n",
    "stoplist = set(stopwords.words(\"english\"))\n",
    "\n",
    "def create_word_features(words):\n",
    "    useful_words = [word for word in words if word not in stoplist] \n",
    "    my_dict = dict([(word, True) for word in useful_words])\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = []\n",
    "for x,y in zip(plots_valid, y_valid):\n",
    "    words = x.split()\n",
    "    cl.append(((create_ngram_features(words)),y))\n",
    "test = []\n",
    "for x,y in zip(plots_test, y_test):\n",
    "    words = x.split()\n",
    "    test.append((create_ngram_features(words),y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from textblob.classifiers import NaiveBayesClassifier\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "clf = NaiveBayesClassifier.train(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.classify.util.accuracy(clf, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features_in_list(classifier, n=10):\n",
    "    \"\"\"\n",
    "    Return a nested list of the \"most informative\" features \n",
    "    used by the classifier along with it's predominant labels\n",
    "    \"\"\"\n",
    "    cpdist = classifier._feature_probdist       # probability distribution for feature values given labels\n",
    "    feature_list = []\n",
    "    for (fname, fval) in classifier.most_informative_features(n):\n",
    "        def labelprob(l):\n",
    "            return cpdist[l, fname].prob(fval)\n",
    "        labels = sorted([l for l in classifier._labels if fval in cpdist[l, fname].samples()], \n",
    "                        key=labelprob)\n",
    "        feature_list.append([fname, labels[-1]])\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feats = show_most_informative_features_in_list(clf,50)\n",
    "for i in feats:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "extractor = ConllExtractor()\n",
    "for i in plots:\n",
    "    blob = TextBlob(i, np_extractor=extractor)\n",
    "    print(blob.noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "with open('np_trec.txt','w') as f:\n",
    "    for i in phrases:\n",
    "        f.write(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph = []\n",
    "for i in phrases:\n",
    "    ph.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "with open('np_trec.txt','r') as f:\n",
    "    for i in f.readlines():\n",
    "        vocab.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab1 = [\"what\"]#,\"does the\",\"what 's\",\"mean ?\",\"are the\",\"stand for\",\"what does\",\"for ?\",\"in ?\",\"other What\",\"was the\",\"what was\",\"do you\",\"does a\",\"in the\",\"the most\",\"the first\",\"is the\",\"on a\",\"did the\",\"name ?\",\"of the\",\"name the\",\"can i\",\"is a\",\"in what\",\"by the\",\"chancellor of\",\"of ?\",\"were the\",\"from the\",\"into the\",\"What are\",\"australia ?\",\"book to\",\"call the\",\"do italians\",\"italians call\",\"the tallest\",\"to help\",\"was hitler\",\"has the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mytokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, decode_error='ignore', strip_accents='ascii', ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32310)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_feats = np.where(np.sum(X,0)> 0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 32307, 32308, 32309])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:,valid_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mytokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_features(val_primitive_matrix, train_primitive_matrix, thresh=0.001):\n",
    "    val_sum = np.sum(np.abs(val_primitive_matrix),axis=0)\n",
    "    train_sum = np.sum(np.abs(train_primitive_matrix),axis=0)\n",
    "\n",
    "    #Only select the indices that fire more than 1% for both datasets\n",
    "    train_idx = np.where((train_sum >= thresh*np.shape(train_primitive_matrix)[0]))[0]\n",
    "    val_idx = np.where((val_sum >= thresh*np.shape(val_primitive_matrix)[0]))[0]\n",
    "    common_idx = list(set(train_idx) & set(val_idx))\n",
    "    return common_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = {'abbreviation':0,'actor':1,'actress':2,'address':3,'age':4,'alias':5,'amount':6,'are':7,'around':8,'at':9,'ate':10,'book':11,'build':12,'built':13,'by':14,'called':15,'can':16,'capital':17,'captain':18,'citizen':19,'close':20,'company':21,'composed':22,'could':23,'country':24,'date':25,'day':26,'demands':27,'describe':28,'did':29,'discovered':30,'division':31,'do':32,'doctor':33,'does':34,'does ':35,'doesn':36,'engineer':37,'enumerate':38,'explain':39,'far':40,'fastener':41,'fastener ':42,'fear':43,'for':44,'found':45,'from':46,'game':47,'gamer':48,'governs':49,'group':50,'groups':51,'guarded':52,'hero':53,'hours':54,'how':55,'human':56,'hypertension':57,'in':58,'instance':59,'invented':60,'is':61,'is ':62,'island':63,'kind':64,'king':65,'latitude':66,'latitude ':67,'lawyer':68,'leader':69,'leads':70,'list':71,'lived':72,'lives':73,'located':74,'long':75,'longitude':76,'made':77,'man':78,'many':79,'mean':80,'meant':81,'minute':82,'model':83,'month':84,'movie':85,'much':86,'name':87,'name ':88,'nationalist':89,'near':90,'nicknamed':91,'novel':92,'number':93,'object':94,'of':95,'old':96,'organization':97,'origin':98,'out':99,'owner':100,'owns':101,'part':102,'patent':103,'pays':104,'percentage':105,'person':106,'play':107,'played':108,'player':109,'poet':110,'population':111,'portrayed':112,'president':113,'queen':114,'ratio':115,'run':116,'seconds':117,'served':118,'shall':119,'share':120,'short':121,'should':122,'should ':123,'situated':124,'slept':125,'small':126,'speed':127,'stand':128,'star':129,'studied':130,'study ':131,'surname':132,'surrounds':133,'take':134,'tall':135,'team':136,'teams':137,'tetrinet':138,'the':139,'thing':140,'through':141,'time':142,'to':143,'trust':144,'unusual':145,'used':146,'using':147,'various':148,'was':149,'was ':150,'watched':151,'what':152,'what ':153,'when':154,'where':155,'where ':156,'which':157,'who':158,'who ':159,'why':160,'wide':161,'will':162,'woman':163,'worked':164,'would':165,'year':166,'you':167}\n",
    "# vocab = {'name a':0,'how does':1,'how to':2,'how can':3,'how should':4,'how would':5,'how could':6,'how will':7,'how do':8,'what is':9,'what fastener':10,'how do you':11,'who person':12,'who man':13,'who woman':14,'who human':15,'who president':16,'what person':17,'what man':18,'what woman':19,'what human':20,'what president':21,'how much':22,'how many':23,'what kind':24,'what amount':25,'what number':26,'what percentage':27,'capital of':28,'why does':29,'why should':30,'why shall':31,'why could':32,'why would':33,'why will':34,'why can':35,'why do':36,'composed from':37,'composed through':38,'composed using':39,'composed by':40,'composed of':41,'made from':42,'made through':43,'made using':44,'made by':45,'made of':46,'where island':47,'which island':48,'what island':49,'who owner':50,'who leads':51,'who governs':52,'who pays':53,'who owns':54,'what is tetrinet':55,'who found':56,'who discovered':57,'who made':58,'who built':59,'who build':60,'who invented':61,'why doesn':62,'used for':63,'when did':64,'when do':65,'when does':66,'when was':67,'how old':68,'how far':69,'how long':70,'how tall':71,'how wide':72,'how short':73,'how small':74,'how close':75,'fear of':76,'explain can':77,'describe can':78,'who worked':79,'who lived':80,'who guarded':81,'who watched':82,'who played':83,'who ate':84,'who slept':85,'who portrayed':86,'who served':87,'what part':88,'what division':89,'what ratio':90,'who is':91,'who will':92,'who was':93,'what do':94,'what does':95,'enumerate the various':96,'list out the various':97,'name the various':98,'at which age':99,'at which year':100,'at how many age':101,'at how many year':102,'at what age':103,'at what year':104,'in which age':105,'in which year':106,'in how many age':107,'in how many year':108,'in what age':109,'in what year':110,'which play':111,'which game':112,'which movie':113,'which book':114,'what play':115,'what game':116,'what movie':117,'what book':118,'which is':119,'which will':120,'which are':121,'which was':122,'who are':123,'by how much':124,'by how many':125,'where was':126,'where is':127,'studied':128,'patent':129,'who':130,'man':131,'swoman':132,'human':133,'person':134,'where':135,'stand':136,'what':137,'mean':138,'meant':139,'called':140,'unusual':141,'origin':142,'country':143,'queen':144,'king':145,'year':146,'novel':147,'speed':148,'abbreviation':149,'percentage':150,'share':151,'number':152,'population':153,'explain':154,'describe':155,'located':156,'thing':157,'instance':158,'object':159,'demands':160,'take':161,'leader':162,'citizen':163,'captain':164,'nationalist':165,'hero':166,'actor':167,'actress':168,'star':169,'gamer':170,'player':171,'lawyer':172,'president':173,'lives':174,'latitude':175,'longitude':176,'alias':177,'nicknamed':178}\n",
    "vectorizer = CountVectorizer(vocabulary=vocab, tokenizer = mytokenizer , ngram_range=(1,2))#, stop_words='english')\n",
    "X = vectorizer.fit_transform(plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(X[:,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(X[:,149])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_primitive_matrix, val_primitive_matrix, test_primitive_matrix, \\\n",
    "        train_ground, val_ground, test_ground,\\\n",
    "        train_plots, val_plots, test_plots = split_data(X, plots, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sum = np.sum(np.abs(val_primitive_matrix),axis=0)\n",
    "train_sum = np.sum(np.abs(train_primitive_matrix),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_feats = np.where(np.sum(X,0)> 2)[1]\n",
    "# X = X[:,valid_feats]\n",
    "\n",
    "\n",
    "train_primitive_matrix, val_primitive_matrix, test_primitive_matrix, \\\n",
    "train_ground, val_ground, test_ground,\\\n",
    "train_plots, val_plots, test_plots = split_data(X, plots, labels)\n",
    "\n",
    "#Prune Feature Space\n",
    "common_idx = prune_features(val_primitive_matrix, train_primitive_matrix)\n",
    "\n",
    "# return train_primitive_matrix[:,common_test_primitive_matrixidx], val_primitive_matrix[:,common_idx], test_primitive_matrix[:,common_idx], \\\n",
    "#         np.array(train_ground), np.array(val_ground), np.array(test_ground), vectorizer, valid_feats, common_idx, \\\n",
    "#     train_plots, val_plots, test_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[vectorizer.get_feature_names()[i] for i in common_idx ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([vectorizer.get_feature_names()[valid_feats[i]] for i in common_idx ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    \"\"\" A class to load in appropriate numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    def prune_features(self, val_primitive_matrix, train_primitive_matrix, thresh=0.01):\n",
    "        val_sum = np.sum(np.abs(val_primitive_matrix),axis=0)\n",
    "        train_sum = np.sum(np.abs(train_primitive_matrix),axis=0)\n",
    "\n",
    "        #Only select the indices that fire more than 1% for both datasets\n",
    "        train_idx = np.where((train_sum >= thresh*np.shape(train_primitive_matrix)[0]))[0]\n",
    "        val_idx = np.where((val_sum >= thresh*np.shape(val_primitive_matrix)[0]))[0]\n",
    "        common_idx = list(set(train_idx) & set(val_idx))\n",
    "\n",
    "        return common_idx\n",
    "    def load_data(self, dataset, data_path='/home/ayusham/Semi_Supervised_LFs/Data/TREC/'):\n",
    "     \n",
    "        plots, labels = parse_file(data_path+'all.txt')\n",
    "        \n",
    "        #Featurize Plots  \n",
    "        vectorizer = CountVectorizer(min_df=1, binary=True, stop_words='english', \\\n",
    "            decode_error='ignore', strip_accents='ascii', ngram_range=(1,2))\n",
    "        X = vectorizer.fit_transform(plots)\n",
    "        valid_feats = np.where(np.sum(X,0)> 2)[1]\n",
    "        X = X[:,valid_feats]\n",
    "\n",
    "#         Split Dataset into Train, Val, Test\n",
    "        train_primitive_matrix, val_primitive_matrix, test_primitive_matrix, \\\n",
    "        train_ground, val_ground, test_ground,\\\n",
    "        train_plots, val_plots, test_plots = split_data(X, plots, labels)\n",
    "\n",
    "        #Prune Feature Space\n",
    "        common_idx = self.prune_features(val_primitive_matrix, train_primitive_matrix)\n",
    "#         print('common_idx',len(common_idx))\n",
    "#         return train_primitive_matrix[:,common_idx], val_primitive_matrix[:,common_idx], test_primitive_matrix[:,common_idx],             np.array(train_ground), np.array(val_ground), np.array(test_ground),train_plots, val_plots, test_plots\n",
    "\n",
    "        return train_primitive_matrix[:,common_idx], val_primitive_matrix[:,common_idx], test_primitive_matrix[:,common_idx], \\\n",
    "                np.array(train_ground), np.array(val_ground), np.array(test_ground), vectorizer, valid_feats, common_idx, \\\n",
    "            train_plots, val_plots, test_plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset is  -f\n",
      "/home/ayusham/auto_lfs/reef/data/trec/all.txt\n",
      "len of data 5965\n",
      "4774 [    0     5     6 ... 36844 36855 36856]\n",
      "common_idx 1094\n"
     ]
    }
   ],
   "source": [
    "# arguments - dataset(1) mode(random/all/normal)(2) model(dt/lr/nn)(3) cardinality(4) num_of_loops(5)\n",
    "# save directory (6)\n",
    "\n",
    "\n",
    "# python generic_generate_labels.py imdb normal dt 1 26 imdb_val2.5_sup5_dt1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from program_synthesis.heuristic_generator import HeuristicGenerator\n",
    "from program_synthesis.synthesizer import Synthesizer\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "from sklearn import model_selection as cross_validation\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "dataset= sys.argv[1]\n",
    "print('dataset is ', dataset)\n",
    "loader_file = \"data.trec_loader\"\n",
    "\n",
    "import importlib\n",
    "\n",
    "load = importlib.import_module(loader_file)\n",
    "\n",
    "dl = load.DataLoader()\n",
    "train_primitive_matrix, val_primitive_matrix, test_primitive_matrix, train_ground,\\\n",
    "    val_ground, test_ground, _, _, _, _,_,_ = dl.load_data(dataset=dataset, split_val = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  3,  1, -1,  2, -1,  2,  3,  3,  3,  4,  3,  3,  3,  3,  2,  2,\n",
       "        3,  4,  1, -1,  3, -1,  3,  4, -1,  4,  4,  1,  2,  1,  1,  3,  1,\n",
       "        4,  4, -1,  4,  4, -1,  2,  4,  2,  3,  2,  4,  4,  4,  1,  3,  1,\n",
       "        2,  3,  3,  2, -1,  4, -1, -1,  2,  2, -1, -1,  2,  5,  2, -1,  3,\n",
       "        2,  2,  4, -1,  1,  5, -1,  2, -1,  3,  1,  4,  1,  4,  2,  2,  3,\n",
       "        3,  1,  3,  3,  3,  1,  2,  3,  2,  3,  4,  4, -1,  1,  1, -1,  2,\n",
       "        1,  1,  2, -1,  2, -1,  3,  1, -1,  1,  4,  3, -1,  1,  2,  4,  2,\n",
       "        4,  3,  1,  4,  2,  3,  1,  3, -1,  4, -1, -1,  2,  3,  3,  3, -1,\n",
       "        3,  3,  2,  2,  3,  2,  4,  4,  2, -1,  4,  1, -1, -1, -1, -1,  2,\n",
       "        2,  4,  1, -1,  1,  1, -1,  3,  4,  3, -1,  4,  4, -1,  1,  1,  1,\n",
       "        4,  3,  3, -1,  2,  3, -1,  4, -1,  2,  2, -1,  1,  3,  3,  1, -1,\n",
       "        1,  2,  1,  3,  2,  1,  2,  3,  4,  2, -1,  1,  4, -1,  4,  3, -1,\n",
       "        2,  4, -1, -1,  3,  3,  3,  2,  1,  3, -1,  2,  2,  4,  4,  3,  4,\n",
       "       -1,  4,  4, -1, -1,  4,  1,  3,  4, -1,  5,  4,  1,  2,  2,  4,  2,\n",
       "        3,  1,  4,  1, -1,  4,  2,  3,  4,  1,  2,  1, -1,  3,  2,  3, -1,\n",
       "        2,  4,  2,  2,  2, -1,  2,  2,  2,  2,  2,  3,  1,  4,  4, -1,  3,\n",
       "        4])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ground[:int(len(val_ground)/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  3,  1, -1,  2, -1,  2,  3,  3,  3,  4,  3,  3,  3,  3,  2,  2,\n",
       "        3,  4,  1, -1,  3, -1,  3,  4, -1,  4,  4,  1,  2,  1,  1,  3,  1,\n",
       "        4,  4, -1,  4,  4, -1,  2,  4,  2,  3,  2,  4,  4,  4,  1,  3,  1,\n",
       "        2,  3,  3,  2, -1,  4, -1, -1,  2,  2, -1, -1,  2,  5,  2, -1,  3,\n",
       "        2,  2,  4, -1,  1,  5, -1,  2, -1,  3,  1,  4,  1,  4,  2,  2,  3,\n",
       "        3,  1,  3,  3,  3,  1,  2,  3,  2,  3,  4,  4, -1,  1,  1, -1,  2,\n",
       "        1,  1,  2, -1,  2, -1,  3,  1, -1,  1,  4,  3, -1,  1,  2,  4,  2,\n",
       "        4,  3,  1,  4,  2,  3,  1,  3, -1,  4, -1, -1,  2,  3,  3,  3, -1,\n",
       "        3,  3,  2,  2,  3,  2,  4,  4,  2, -1,  4,  1, -1, -1, -1, -1,  2,\n",
       "        2,  4,  1, -1,  1,  1, -1,  3,  4,  3, -1,  4,  4, -1,  1,  1,  1,\n",
       "        4,  3,  3, -1,  2,  3, -1,  4, -1,  2,  2, -1,  1,  3,  3,  1, -1,\n",
       "        1,  2,  1,  3,  2,  1,  2,  3,  4,  2, -1,  1,  4, -1,  4,  3, -1,\n",
       "        2,  4, -1, -1,  3,  3,  3,  2,  1,  3, -1,  2,  2,  4,  4,  3,  4,\n",
       "       -1,  4,  4, -1, -1,  4,  1,  3,  4, -1,  5,  4,  1,  2,  2,  4,  2,\n",
       "        3,  1,  4,  1, -1,  4,  2,  3,  4,  1,  2,  1, -1,  3,  2,  3, -1,\n",
       "        2,  4,  2,  2,  2, -1,  2,  2,  2,  2,  2,  3,  1,  4,  4, -1,  3,\n",
       "        4])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ground[:int(len(val_ground)/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  1, -1,  4,  2, -1,  3,  3,  3,  4,  3,  1,  1,  4,  1,  4,  1,\n",
       "        1,  2,  3,  3, -1,  2,  2,  1, -1,  4,  2, -1,  4,  4,  2,  3,  2,\n",
       "       -1,  2,  2,  2,  2,  2,  3,  1,  3,  2, -1,  1,  3,  4,  4,  2,  4,\n",
       "       -1,  4, -1,  1,  4,  4,  4,  3,  1,  1, -1,  4, -1, -1,  4,  4,  2,\n",
       "        4,  4,  4,  4,  4,  5,  3,  3,  1,  5,  2,  2, -1,  2,  4,  2,  4,\n",
       "        4,  4, -1,  1,  1,  2,  4, -1,  3, -1,  1, -1, -1,  2,  4, -1, -1,\n",
       "       -1,  4,  4,  4,  1,  3,  3, -1,  2, -1, -1,  3,  1,  2,  3, -1, -1,\n",
       "        1,  1,  4,  3,  3,  2,  1,  1,  4,  1,  4,  2,  2,  3, -1,  2,  2,\n",
       "       -1,  4,  2, -1,  4,  2, -1, -1,  3,  3, -1,  1,  4,  3,  4,  3,  5,\n",
       "        1,  2,  3,  4,  2,  2,  2,  3,  3,  4,  4,  4,  4,  1, -1,  2,  2,\n",
       "        2,  2,  3, -1,  1,  3,  1,  3,  2,  3,  2,  2,  4,  2,  4,  1,  3,\n",
       "        3,  4,  2,  3,  2,  2,  5,  3,  2, -1,  2,  4,  2, -1,  1,  3,  1,\n",
       "        2,  1,  1,  4,  2,  4,  1,  2,  4,  5,  4,  4,  4,  4,  2,  4,  4,\n",
       "        3,  3,  2,  3,  1,  2,  4, -1,  2, -1,  3,  4,  2,  1,  4, -1,  3,\n",
       "        2,  2,  4,  3,  2,  2,  1, -1,  2,  2,  3,  2, -1, -1,  2,  1,  3,\n",
       "        2,  4,  2,  4,  1,  1,  4,  2,  2,  4,  4,  2,  3,  3,  2,  4, -1,\n",
       "        4,  4, -1,  4,  1,  3,  1,  2,  4,  3,  1,  4,  2,  3,  3,  2,  4,\n",
       "        5,  5, -1,  2,  4,  2,  4,  5,  4, -1,  4, -1,  2, -1,  3,  3,  1,\n",
       "       -1,  1,  3,  3, -1,  1,  3, -1,  4,  2,  2,  4,  3,  4,  4, -1, -1,\n",
       "        4,  1, -1,  4,  4,  4,  2,  3,  4,  3,  4, -1, -1,  4,  4,  4,  2,\n",
       "        3,  3,  1,  2,  4,  1,  1, -1, -1,  3,  3,  4, -1,  4, -1,  3,  2,\n",
       "       -1, -1,  2, -1,  1,  3,  3,  1,  2,  1,  4,  1, -1,  2,  3,  3,  4,\n",
       "        3,  3, -1,  1,  3, -1,  3,  3, -1,  4,  4,  1,  1, -1,  1,  4,  4,\n",
       "        2,  4,  2,  2,  4,  3,  2,  1,  3,  1,  2,  3,  3,  2,  2,  1, -1,\n",
       "       -1,  3,  3,  4,  3,  3, -1,  1,  4, -1,  4, -1,  2, -1,  2,  4,  2,\n",
       "        3, -1,  2,  3,  4,  3,  1,  4,  1,  2,  2,  2,  2,  1,  3,  4,  3,\n",
       "        3,  2,  3,  2,  4,  4,  3,  3,  3,  3,  3,  3,  2, -1,  1,  2,  2,\n",
       "        3,  1,  1,  3,  4,  4, -1,  1,  1, -1,  4,  3,  2,  1,  3,  3,  4,\n",
       "       -1,  1,  4,  4,  4,  1,  3,  4,  3,  4,  3,  4,  1, -1,  1,  4,  1,\n",
       "        4,  2,  1,  1,  3,  1, -1,  3,  4,  4,  1,  4,  2,  3,  3,  3,  1,\n",
       "       -1,  3,  3,  3,  1,  2,  1,  2, -1,  2,  1,  2,  3,  2,  3,  3,  3,\n",
       "        3,  4,  3,  2,  2,  2,  4,  3,  1,  2, -1,  1,  4,  2,  4,  3,  3,\n",
       "        4,  3,  1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ground"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
